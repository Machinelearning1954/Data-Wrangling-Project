{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebcfcf9",
   "metadata": {},
   "source": [
    "# Machine Learning Engineering Bootcamp Capstone: Data Wrangling\n",
    "\n",
    "This notebook documents the process of data wrangling for the capstone project. It includes collecting data from multiple sources, cleaning the data, addressing missing values and outliers, and preparing the data for machine learning modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e2922",
   "metadata": {},
   "source": [
    "## Step 1: Collect and Explore Datasets\n",
    "\n",
    "In this step, we collect data from two disparate sources: Apple (AAPL) stock data from Yahoo Finance API and US Gross Domestic Product (GDP) data from the World Bank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0975a7",
   "metadata": {},
   "source": [
    "### 1.1 Apple (AAPL) Stock Data\n",
    "\n",
    "The AAPL stock data was fetched using the `YahooFinance/get_stock_chart` API for a 5-year range with daily intervals. It includes timestamps, open, high, low, close (OHLC) values, volume, and adjusted close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load AAPL stock data\n",
    "with open(\"/home/ubuntu/data/aapl_stock_data.json\", \"r\") as f:\n",
    "    aapl_data_raw = json.load(f)\n",
    "\n",
    "# Initial exploration of AAPL stock data structure\n",
    "print(\"AAPL Stock Data Keys:\", aapl_data_raw.keys())\n",
    "if \"chart\" in aapl_data_raw and \"result\" in aapl_data_raw[\"chart\"] and len(aapl_data_raw[\"chart\"][\"result\"]) > 0:\n",
    "    print(\"Meta Data Keys:\", aapl_data_raw[\"chart\"][\"result\"][0][\"meta\"].keys())\n",
    "    print(\"Number of timestamps:\", len(aapl_data_raw[\"chart\"][\"result\"][0][\"timestamp\"]))\n",
    "\n",
    "    print(\"Indicators Keys:\", aapl_data_raw[\"chart\"][\"result\"][0][\"indicators\"].keys())\n",
    "    print(\"Quote Keys:\", aapl_data_raw[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0].keys())\n",
    "    print(\"Adjclose Keys:\", aapl_data_raw[\"chart\"][\"result\"][0][\"indicators\"][\"adjclose\"][0].keys())\n",
    "    \n",
    "    # Convert to DataFrame for easier handling later\n",
    "    timestamps = aapl_data_raw[\"chart\"][\"result\"][0][\"timestamp\"]\n",
    "    ohlcv = aapl_data_raw[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0]\n",
    "    adjclose = aapl_data_raw[\"chart\"][\"result\"][0][\"indicators\"][\"adjclose\"][0][\"adjclose\"]\n",
    "    \n",
    "    aapl_df = pd.DataFrame({\n",
    "        \"timestamp\": timestamps,\n",
    "        \"open\": ohlcv[\"open\"] if \"open\" in ohlcv else [np.nan] * len(timestamps),\n",
    "        \"high\": ohlcv[\"high\"] if \"high\" in ohlcv else [np.nan] * len(timestamps),\n",
    "        \"low\": ohlcv[\"low\"] if \"low\" in ohlcv else [np.nan] * len(timestamps),\n",
    "        \"close\": ohlcv[\"close\"] if \"close\" in ohlcv else [np.nan] * len(timestamps),\n",
    "        \"volume\": ohlcv[\"volume\"] if \"volume\" in ohlcv else [np.nan] * len(timestamps),\n",
    "        \"adjclose\": adjclose if adjclose else [np.nan] * len(timestamps)\n",
    "    })\n",
    "    aapl_df[\"date\"] = pd.to_datetime(aapl_df[\"timestamp\"], unit=\"s\")\n",
    "    print(\"AAPL Stock Data (first 5 rows):\")\n",
    "    print(aapl_df.head())\n",
    "    print(\"AAPL Stock Data Info:\")\n",
    "    aapl_df.info()\n",
    "else:\n",
    "    print(\"AAPL stock data is not in the expected format or is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770dc74",
   "metadata": {},
   "source": [
    "### 1.2 US GDP Data\n",
    "\n",
    "The US GDP data (Indicator: NY.GDP.MKTP.CD, GDP current US$) was initially planned to be fetched via the World Bank API. Due to an API authentication issue, an alternative approach was taken: downloading the data as a CSV file directly from the World Bank data portal.\n",
    "\n",
    "The downloaded ZIP file contained multiple CSVs. The relevant data is in `API_NY.GDP.MKTP.CD_DS2_en_csv_v2_85078.csv`. This file contains GDP data for all countries. We will need to filter it for the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05888a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US GDP data from CSV\n",
    "gdp_csv_path = \"/home/ubuntu/data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_85078.csv\"\n",
    "\n",
    "# The CSV has some metadata rows at the top. We need to skip them.\n",
    "# Based on previous inspection, the actual data starts from the 5th row (index 4), header is on this line.\n",
    "try:\n",
    "    gdp_raw_df = pd.read_csv(gdp_csv_path, skiprows=4)\n",
    "    print(\"US GDP Data (first 5 rows of raw data):\")\n",
    "    print(gdp_raw_df.head())\n",
    "    print(\"US GDP Data Columns:\")\n",
    "    print(gdp_raw_df.columns)\n",
    "    \n",
    "    # Filter for United States\n",
    "    us_gdp_df_wide = gdp_raw_df[gdp_raw_df[\"Country Name\"] == \"United States\"].copy()\n",
    "    print(\"US GDP Data (United States only - Wide Format):\")\n",
    "    print(us_gdp_df_wide)\n",
    "    print(\"US GDP Data (United States - Wide Format) Info:\")\n",
    "    us_gdp_df_wide.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: GDP CSV file not found at {gdp_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ca614",
   "metadata": {},
   "source": [
    "### 1.3 Initial Data Exploration Summary\n",
    "\n",
    "**AAPL Stock Data:**\n",
    "- Contains daily OHLCV and adjusted close prices for AAPL for the last 5 years.\n",
    "- Timestamps are provided and converted to datetime objects.\n",
    "- Data appears suitable for time-series analysis and merging with annual GDP data (after appropriate aggregation/alignment).\n",
    "\n",
    "**US GDP Data:**\n",
    "- Contains annual GDP data (current US$) for many countries, sourced from the World Bank.\n",
    "- Filtered to retain only data for the \"United States\".\n",
    "- The data is in a wide format, with years as columns. This will need to be reshaped (melted) into a long format for easier use.\n",
    "- Contains data from 1960 to 2023 (or latest available year)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2f4d7",
   "metadata": {},
   "source": [
    "## Step 2: Clean and Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fd78f",
   "metadata": {},
   "source": [
    "### 2.1 Clean AAPL Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad54963",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking for missing values in AAPL data:\")\n",
    "print(aapl_df.isnull().sum())\n",
    "\n",
    "# Handle missing values. Financial data often has NaNs if trading didn\"t occur or data wasn\"t recorded.\n",
    "price_cols = [\"open\", \"high\", \"low\", \"close\", \"adjclose\"]\n",
    "for col in price_cols:\n",
    "    if aapl_df[col].isnull().all():\n",
    "        print(f\"Warning: Column {col} is entirely NaN.\")\n",
    "    elif aapl_df[col].isnull().any():\n",
    "        aapl_df[col] = aapl_df[col].fillna(method=\"ffill\")\n",
    "        aapl_df[col] = aapl_df[col].fillna(method=\"bfill\")\n",
    "\n",
    "if aapl_df[\"volume\"].isnull().any():\n",
    "    aapl_df[\"volume\"] = aapl_df[\"volume\"].fillna(0)\n",
    "\n",
    "print(\"Missing values in AAPL data after handling:\")\n",
    "print(aapl_df.isnull().sum())\n",
    "\n",
    "aapl_df.dropna(subset=[\"date\", \"close\"], inplace=True)\n",
    "\n",
    "print(\"AAPL Data Description after handling missing values:\")\n",
    "print(aapl_df[price_cols + [\"volume\"]].describe())\n",
    "\n",
    "print(\"AAPL Data types:\")\n",
    "print(aapl_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f066f3f",
   "metadata": {},
   "source": [
    "### 2.2 Clean US GDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d49623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape US GDP data from wide to long format\n",
    "id_vars = [\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\"]\n",
    "value_vars = [col for col in us_gdp_df_wide.columns if col.isdigit()]\n",
    "\n",
    "us_gdp_long = pd.melt(us_gdp_df_wide, \n",
    "                        id_vars=id_vars, \n",
    "                        value_vars=value_vars, \n",
    "                        var_name=\"Year\", \n",
    "                        value_name=\"GDP_USD\")\n",
    "\n",
    "print(\"US GDP Data (Long Format - First 5 rows):\")\n",
    "print(us_gdp_long.head())\n",
    "\n",
    "us_gdp_long[\"Year\"] = pd.to_numeric(us_gdp_long[\"Year\"])\n",
    "us_gdp_long[\"GDP_USD\"] = pd.to_numeric(us_gdp_long[\"GDP_USD\"], errors=\"coerce\")\n",
    "\n",
    "print(\"Missing GDP values before handling:\")\n",
    "print(us_gdp_long.isnull().sum())\n",
    "\n",
    "us_gdp_long.sort_values(by=\"Year\", inplace=True)\n",
    "us_gdp_long.dropna(subset=[\"GDP_USD\"], inplace=True)\n",
    "\n",
    "print(\"Missing GDP values after handling (dropping NaNs):\")\n",
    "print(us_gdp_long.isnull().sum())\n",
    "\n",
    "print(\"US GDP Data (Cleaned - First 5 rows):\")\n",
    "print(us_gdp_long.head())\n",
    "print(\"US GDP Data (Cleaned - Last 5 rows):\")\n",
    "print(us_gdp_long.tail())\n",
    "print(\"US GDP Data (Cleaned) Info:\")\n",
    "print(us_gdp_long.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccba882",
   "metadata": {},
   "source": [
    "### 2.3 Merge AAPL Stock Data and US GDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare AAPL data for merging: add a \"Year\" column\n",
    "aapl_df[\"Year\"] = aapl_df[\"date\"].dt.year\n",
    "\n",
    "# Select relevant columns from GDP data for merging\n",
    "gdp_to_merge = us_gdp_long[[\"Year\", \"GDP_USD\"]].copy()\n",
    "\n",
    "merged_df = pd.merge(aapl_df, gdp_to_merge, on=\"Year\", how=\"left\")\n",
    "\n",
    "print(\"Merged Data (First 5 rows):\")\n",
    "print(merged_df.head())\n",
    "print(\"Merged Data (Last 5 rows):\")\n",
    "print(merged_df.tail())\n",
    "\n",
    "print(\"Missing values in Merged Data:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "print(\"Merged Data Info:\")\n",
    "merged_df.info()\n",
    "\n",
    "merged_df.to_csv(\"/home/ubuntu/data/aapl_gdp_merged_cleaned.csv\", index=False)\n",
    "print(\"Cleaned and merged data saved to /home/ubuntu/data/aapl_gdp_merged_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49680522",
   "metadata": {},
   "source": [
    "## Step 3: Perform Data Wrangling and Feature Engineering\n",
    "\n",
    "In this step, we will create new features from the existing data to potentially improve model performance. We will also consider any further data transformations needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and merged dataset\n",
    "merged_df = pd.read_csv(\"/home/ubuntu/data/aapl_gdp_merged_cleaned.csv\")\n",
    "# Convert date back to datetime object as CSV doesn\"t store type information perfectly\n",
    "merged_df[\"date\"] = pd.to_datetime(merged_df[\"date\"])\n",
    "\n",
    "print(\"Loaded merged_df for feature engineering (first 5 rows):\")\n",
    "print(merged_df.head())\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f84a5",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering for AAPL Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Daily Returns for adjclose\n",
    "merged_df[\"daily_return\"] = merged_df[\"adjclose\"].pct_change()\n",
    "\n",
    "# Calculate Moving Averages for adjclose\n",
    "merged_df[\"MA7_adjclose\"] = merged_df[\"adjclose\"].rolling(window=7).mean()\n",
    "merged_df[\"MA30_adjclose\"] = merged_df[\"adjclose\"].rolling(window=30).mean()\n",
    "\n",
    "# Calculate Rolling Volatility (standard deviation of daily returns)\n",
    "merged_df[\"volatility30\"] = merged_df[\"daily_return\"].rolling(window=30).std()\n",
    "\n",
    "print(\"Dataframe with stock features (first 35 rows to see some MA/volatility values):\")\n",
    "print(merged_df.head(35))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc59d4",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering for US GDP Data\n",
    "\n",
    "We want to calculate the Year-over-Year (YoY) GDP growth rate. Since GDP data is annual and already merged, we can calculate this on the `GDP_USD` column. We need to be careful as GDP values are repeated for each day within a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate YoY GDP Growth Rate\n",
    "# First, get unique year-GDP pairs to avoid issues with daily repetition\n",
    "gdp_yearly = merged_df[[\"Year\", \"GDP_USD\"]].drop_duplicates().sort_values(by=\"Year\")\n",
    "gdp_yearly[\"GDP_growth_YoY\"] = gdp_yearly[\"GDP_USD\"].pct_change()\n",
    "\n",
    "# Merge this back into the main dataframe\n",
    "merged_df = pd.merge(merged_df, gdp_yearly[[\"Year\", \"GDP_growth_YoY\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "print(\"Dataframe with GDP Growth YoY (showing transitions between years):\")\n",
    "# Find indices where year changes to display relevant parts\n",
    "year_change_indices = merged_df[merged_df[\"Year\"] != merged_df[\"Year\"].shift(1)].index\n",
    "display_indices = sorted(list(set(year_change_indices.tolist() + [i-1 for i in year_change_indices if i>0] + [i+1 for i in year_change_indices if i < len(merged_df)-1])))\n",
    "if len(display_indices) > 40: display_indices = display_indices[:20] + display_indices[-20:] # Limit display if too many transitions\n",
    "print(merged_df.loc[display_indices, [\"date\", \"Year\", \"GDP_USD\", \"GDP_growth_YoY\"]])\n",
    "\n",
    "print(\"Missing values after feature engineering:\")\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ff7a3",
   "metadata": {},
   "source": [
    "### 3.3 Further Data Wrangling Considerations (Examples)\n",
    "\n",
    "- **Lagged Features:** For time-series forecasting, lagged versions of stock prices or returns (e.g., previous day\"s close, return from  T-1, T-2 days) are often crucial. These can be created using `.shift()`.\n",
    "- **Interaction Features:** Combining stock-specific features with macroeconomic features (e.g., stock volatility during high/low GDP growth periods, though this requires careful definition).\n",
    "- **Date-based Features:** Extracting month, day of the week, quarter from the \"date\" column might be useful for some models to capture seasonality, though for daily stock data, more direct time-series models are common.\n",
    "\n",
    "For this project, the current set of engineered features (daily return, MAs, volatility, GDP growth) provides a good enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff02099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Lagged feature for adjclose\n",
    "merged_df[\"adjclose_lag1\"] = merged_df[\"adjclose\"].shift(1)\n",
    "\n",
    "# Example: Date-based features\n",
    "merged_df[\"month\"] = merged_df[\"date\"].dt.month\n",
    "merged_df[\"day_of_week\"] = merged_df[\"date\"].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "print(\"Dataframe with additional example features (first 5 rows):\")\n",
    "print(merged_df[[\"date\", \"adjclose\", \"adjclose_lag1\", \"month\", \"day_of_week\"]].head())\n",
    "\n",
    "# Final check on missing values after all feature engineering\n",
    "# We will fill NaNs created by .pct_change() and .rolling() with 0 or backfill, as appropriate.\n",
    "# For daily_return, first NaN is legitimate. For MAs and volatility, NaNs at the start are expected.\n",
    "# For GDP_growth_YoY, the first year will have NaN.\n",
    "# For adjclose_lag1, the first row will have NaN.\n",
    "# For simplicity in this stage, we will fill these with 0, but in a real scenario, more nuanced handling might be needed (e.g., bfill for MAs, or dropping initial rows).\n",
    "cols_to_fill_zero = [\"daily_return\", \"MA7_adjclose\", \"MA30_adjclose\", \"volatility30\", \"GDP_growth_YoY\", \"adjclose_lag1\"]\n",
    "for col in cols_to_fill_zero:\n",
    "    merged_df[col] = merged_df[col].fillna(0) # Or use .bfill() for some\n",
    "\n",
    "print(\"Final missing values count after filling NaNs from feature engineering:\")\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a29cc9",
   "metadata": {},
   "source": [
    "### 3.4 Data Standardization/Normalization (Placeholder)\n",
    "\n",
    "Standardization (e.g., using `StandardScaler` from scikit-learn to give data zero mean and unit variance) or Normalization (e.g., `MinMaxScaler` to scale data between 0 and 1) is often a preprocessing step for many machine learning algorithms. \n",
    "\n",
    "The choice of whether and how to scale data depends on the specific algorithm being used. For instance, tree-based models are often insensitive to feature scaling, while distance-based models (like KNN, SVM) and neural networks usually benefit from it.\n",
    "\n",
    "For this data wrangling stage, we will not apply scaling yet, but it\"s an important consideration before model training. We would typically apply scaling only to the training set and then use the fitted scaler to transform the test set to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1e091",
   "metadata": {},
   "source": [
    "### 3.5 Save Data with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a177c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with engineered features\n",
    "wrangled_df_path = \"/home/ubuntu/data/aapl_gdp_wrangled_features.csv\"\n",
    "merged_df.to_csv(wrangled_df_path, index=False)\n",
    "print(f\"Dataframe with engineered features saved to {wrangled_df_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744f9c1",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Data to Inform Decisions and Document Process\n",
    "\n",
    "Visualizations help in understanding data distributions, relationships between variables, and the impact of cleaning and wrangling steps. They can also guide further decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wrangled dataset for visualization\n",
    "df_viz = pd.read_csv(wrangled_df_path)\n",
    "df_viz[\"date\"] = pd.to_datetime(df_viz[\"date\"])\n",
    "df_viz.set_index(\"date\", inplace=True) # Set date as index for time series plots\n",
    "\n",
    "print(\"Dataset for visualization (first 5 rows):\")\n",
    "print(df_viz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09ae5c",
   "metadata": {},
   "source": [
    "### 4.1 Time Series Plots of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de53486",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_viz[\"adjclose\"], label=\"AAPL Adjusted Close\")\n",
    "plt.plot(df_viz[\"MA7_adjclose\"], label=\"7-Day Moving Average\")\n",
    "plt.plot(df_viz[\"MA30_adjclose\"], label=\"30-Day Moving Average\")\n",
    "plt.title(\"AAPL Adjusted Close Price and Moving Averages\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/home/ubuntu/notebooks/aapl_adjclose_ma.png\")\n",
    "# plt.show() # Commented out to avoid blocking in automated execution, images are saved.\n",
    "plt.close() # Add this to free up memory and prevent plots from overlapping in output\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_viz[\"volume\"], label=\"AAPL Volume\")\n",
    "plt.title(\"AAPL Trading Volume\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/home/ubuntu/notebooks/aapl_volume.png\")\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_viz[\"daily_return\"], label=\"AAPL Daily Return\")\n",
    "plt.title(\"AAPL Daily Returns\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/home/ubuntu/notebooks/aapl_daily_return.png\")\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_viz[\"volatility30\"], label=\"30-Day Rolling Volatility\")\n",
    "plt.title(\"AAPL 30-Day Rolling Volatility of Daily Returns\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/home/ubuntu/notebooks/aapl_volatility.png\")\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b00d49",
   "metadata": {},
   "source": [
    "### 4.2 GDP Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot US GDP over time\n",
    "gdp_plot_df = df_viz[[\"Year\", \"GDP_USD\"]].reset_index().drop_duplicates(subset=[\"Year\"]).set_index(\"Year\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(gdp_plot_df.index, gdp_plot_df[\"GDP_USD\"] / 1e12, marker=\".\", linestyle=\"-\") # GDP in Trillions USD\n",
    "plt.title(\"US Nominal GDP Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"GDP (Trillions USD)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/home/ubuntu/notebooks/us_gdp_time_series.png\")\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot US GDP Growth Rate YoY\n",
    "gdp_growth_plot_df = df_viz[[\"Year\", \"GDP_growth_YoY\"]].reset_index().drop_duplicates(subset=[\"Year\"]).set_index(\"Year\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(gdp_growth_plot_df.index, gdp_growth_plot_df[\"GDP_growth_YoY\"] * 100) # Growth rate in percentage\n",
    "plt.title(\"US GDP Growth Rate (YoY)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"GDP Growth Rate (%)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/home/ubuntu/notebooks/us_gdp_growth_rate.png\")\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9338af",
   "metadata": {},
   "source": [
    "### 4.3 Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_viz[\"daily_return\"].dropna(), kde=True, bins=50)\n",
    "plt.title(\"Distribution of AAPL Daily Returns\")\n",
    "plt.xlabel(\"Daily Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"/home/ubuntu/notebooks/aapl_daily_return_distribution.png\")\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_viz[\"adjclose\"].dropna(), kde=True, bins=50)\n",
    "plt.title(\"Distribution of AAPL Adjusted Close Prices\")\n",
    "plt.xlabel(\"Adjusted Close Price (USD)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"/home/ubuntu/notebooks/aapl_adjclose_distribution.png\")\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61857a9",
   "metadata": {},
   "source": [
    "### 4.4 Correlation Analysis (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of numerical features for correlation analysis\n",
    "correlation_features = [\"adjclose\", \"volume\", \"daily_return\", \"volatility30\", \"GDP_USD\", \"GDP_growth_YoY\"]\n",
    "correlation_matrix = df_viz[correlation_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Selected Features\")\n",
    "plt.savefig(\"/home/ubuntu/notebooks/correlation_matrix.png\")\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6244b6",
   "metadata": {},
   "source": [
    "### 4.5 Visualizations Summary\n",
    "\n",
    "The visualizations provide insights into:\n",
    "- **Stock Trends:** AAPL\"s adjusted close price shows an upward trend over the 5-year period, with fluctuations. Moving averages help smooth out short-term volatility and indicate longer-term trends. Trading volume varies, with spikes potentially corresponding to significant news or events. Daily returns are centered around zero, with some periods of higher volatility.\n",
    "- **GDP Trends:** US Nominal GDP shows a consistent upward trend. The YoY GDP growth rate fluctuates, showing periods of economic expansion and contraction (e.g., a dip around 2020, likely due to the COVID-19 pandemic).\n",
    "- **Distributions:** The distribution of daily returns appears somewhat leptokurtic (fat tails), common for financial returns, indicating a higher probability of extreme values than a normal distribution. Adjusted close prices show a multi-modal distribution reflecting price levels over time.\n",
    "- **Correlations:** The example correlation matrix provides a quantitative look at linear relationships. For instance, `adjclose` is highly correlated with its moving averages (as expected). The correlation between daily stock metrics and annual GDP/GDP growth is generally low, which is also expected given the difference in data frequency and the multitude of factors affecting stock prices daily.\n",
    "\n",
    "These visualizations confirm the data is behaving as expected for financial and macroeconomic series and that the cleaning and feature engineering steps have produced reasonable results. No immediate further wrangling steps are suggested solely from these plots, but they provide a good foundation for understanding the data before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70453795",
   "metadata": {},
   "source": [
    "## Step 5: Validate Data Quality and Decisions\n",
    "\n",
    "This section reviews the data wrangling process, justifies the key decisions made, and assesses the overall quality and suitability of the final dataset for machine learning modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1499d",
   "metadata": {},
   "source": [
    "### 5.1 Review of Data Wrangling Steps and Decisions\n",
    "\n",
    "1.  **Data Collection:**\n",
    "    *   **AAPL Stock Data:** Fetched via Yahoo Finance API (`YahooFinance/get_stock_chart`). This is a reliable source for historical stock data. Daily data for 5 years was chosen to capture sufficient history for time-series analysis while remaining manageable.\n",
    "    *   **US GDP Data:** Initially attempted via World Bank API (`DataBank/indicator_data`). An API authentication failure led to sourcing the data from a CSV download from the World Bank data portal (`API_NY.GDP.MKTP.CD_DS2_en_csv_v2_85078.csv`). This maintained the goal of using a disparate, authoritative source for macroeconomic context. The specific indicator (Nominal GDP in current USD) was chosen for its direct relevance to economic scale.\n",
    "\n",
    "2.  **Data Cleaning:**\n",
    "    *   **AAPL Stock Data:**\n",
    "        *   *Missing Values:* Handled by forward-filling (ffill) then backward-filling (bfill) for OHLC and adjusted close prices. This is a common approach for financial time series, assuming prices carry over during non-trading periods or brief data gaps. Volume NaNs were filled with 0, assuming no trades. Rows with critical missing data (e.g., date, close) after filling were set to be dropped, though ffill/bfill typically handles most cases in dense stock data.\n",
    "        *   *Outliers:* No explicit outlier removal was performed on stock prices. Financial data can have legitimate large jumps (e.g., due to earnings announcements, market shocks). Standard outlier removal techniques (like IQR or Z-score based) might incorrectly remove valid data points. The focus was on handling missing data and ensuring correct data types. Visual inspection of price and return plots did not reveal obvious erroneous outliers that would necessitate removal beyond what the source API provides.\n",
    "    *   **US GDP Data:**\n",
    "        *   *Reshaping:* The raw CSV data was in a wide format (years as columns). It was reshaped into a long format (Year and GDP_USD columns) using `pd.melt` for easier analysis and merging.\n",
    "        *   *Data Types:* Year was converted to numeric. GDP_USD was converted to numeric, with non-convertible values becoming NaN.\n",
    "        *   *Missing Values:* Rows with missing GDP_USD values after conversion were dropped. For annual data, interpolation could be an option if gaps are few and internal, but dropping ensures we only use reported figures.\n",
    "\n",
    "3.  **Data Merging:**\n",
    "    *   AAPL stock data (daily) was merged with US GDP data (annual) using a left merge on the `Year` column. This ensures all stock data points are retained, and the corresponding annual GDP is mapped to each day of that year. This is a standard way to combine data of different frequencies when the lower-frequency data provides context for the higher-frequency data.\n",
    "\n",
    "4.  **Feature Engineering:**\n",
    "    *   **Stock-Specific Features:**\n",
    "        *   `daily_return`: Percentage change in adjusted close price. Fundamental for financial analysis.\n",
    "        *   `MA7_adjclose`, `MA30_adjclose`: 7-day and 30-day moving averages of adjusted close. Common technical indicators to smooth price data and identify trends.\n",
    "        *   `volatility30`: 30-day rolling standard deviation of daily returns. Measures price variability.\n",
    "        *   `adjclose_lag1`: Previous day\"s adjusted close price. Essential for many time-series forecasting models.\n",
    "    *   **GDP-Specific Features:**\n",
    "        *   `GDP_growth_YoY`: Year-over-Year percentage change in GDP. Provides a measure of economic momentum.\n",
    "    *   **Date-based Features:**\n",
    "        *   `month`, `day_of_week`: Extracted for potential seasonality analysis, though their utility depends on the chosen model.\n",
    "    *   *Handling NaNs from Feature Engineering:* NaNs generated by `pct_change()` and `rolling()` (at the start of the series) were filled with 0. This is a simplification; in a rigorous modeling scenario, one might drop these initial rows or use more sophisticated imputation if the period is critical.\n",
    "\n",
    "5.  **Data Visualization:**\n",
    "    *   Visualizations were used to explore trends, distributions, and relationships (e.g., stock price over time, GDP growth, return distributions, correlation matrix). These plots helped confirm that the data transformations were sensible and that the data exhibits expected characteristics (e.g., stock price trends, GDP growth cycles, fat tails in returns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412955b",
   "metadata": {},
   "source": [
    "### 5.2 Assessment of Data Quality and Suitability for ML\n",
    "\n",
    "*   **Completeness:** Missing values have been addressed in a reasoned manner. The primary stock data is quite complete after ffill/bfill. GDP data is annual, so its application to daily stock data results in repeated values for GDP within a year, which is an accepted way to incorporate lower-frequency macro data.\n",
    "*   **Consistency & Accuracy:** Data is sourced from reputable providers (Yahoo Finance, World Bank). Transformations (reshaping, merging) were checked for logical consistency. Calculations for engineered features (returns, MAs, growth rates) are standard. The visualizations did not reveal inconsistencies that would question the accuracy of the transformations.\n",
    "*   **Relevance:** The chosen features (stock OHLCV, returns, volatility, MAs, GDP, GDP growth) are relevant for analyses that might involve predicting stock movements or understanding their relationship with macroeconomic indicators. The merging of disparate data sources (stock market and national economic data) enhances the dataset\"s richness for such tasks.\n",
    "*   **Structure for ML:** The final dataset is in a tabular format (CSV), with each row representing a trading day and columns representing various features. This is a standard structure suitable for many ML algorithms. Timestamps are available, which is crucial for time-series modeling.\n",
    "*   **Limitations & Further Considerations:**\n",
    "    *   *Outlier Handling for Stocks:* As mentioned, a more sophisticated domain-specific approach to outlier detection for stock prices might be considered in a production system, though it\"s often complex.\n",
    "    *   *Stationarity:* For time-series forecasting, features (especially the target variable like price or return) often need to be stationary. This was not explicitly addressed in the wrangling phase but would be a key step in pre-modeling data preparation (e.g., by differencing prices, or using returns which are often more stationary).\n",
    "    *   *Look-ahead Bias:* Care was taken in feature engineering (e.g., using `pct_change()` and `rolling()` without future data) to avoid look-ahead bias. When creating lagged features, `.shift()` correctly uses past data.\n",
    "    *   *GDP Data Granularity:* Using annual GDP data for daily stock analysis means the GDP figure is constant for all trading days within a year. While this provides context, higher-frequency economic indicators (e.g., quarterly GDP, monthly unemployment) could offer more dynamic macro insights if the modeling goal required it.\n",
    "    *   *NaN Filling for Engineered Features:* Filling initial NaNs from rolling calculations with 0 is a simple approach. Depending on the model, dropping these rows or using a more careful backfill/interpolation might be preferred to avoid introducing artificial zeros.\n",
    "\n",
    "**Overall, the data wrangling process has resulted in a dataset that is significantly cleaner, richer, and better structured for potential machine learning applications compared to the raw sources. The decisions made were aimed at balancing thoroughness with practicality for this capstone project, demonstrating an understanding of common data wrangling techniques and considerations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5e8c6",
   "metadata": {},
   "source": [
    "## Step 6: Prepare and Upload Cleaned Data and Code to GitHub (Placeholder)\n",
    "\n",
    "This section would typically involve:\n",
    "1.  Ensuring the Jupyter notebook (`data_wrangling.ipynb`) is well-documented with clear explanations for each step.\n",
    "2.  Creating a `README.md` for the GitHub repository. This file should describe:\n",
    "    *   The project and its objectives (focusing on data wrangling for this phase).\n",
    "    *   The data sources used (AAPL stock data from Yahoo Finance, US GDP data from World Bank) and how they were obtained.\n",
    "    *   The structure of the repository (e.g., `/data` for datasets, `/notebooks` for the Jupyter notebook).\n",
    "    *   Instructions on how to run the notebook or reproduce the results (e.g., Python version, required libraries - which can be listed in a `requirements.txt` file).\n",
    "3.  Organizing files into a clear directory structure:\n",
    "    *   `/data/aapl_stock_data.json` (raw downloaded stock data)\n",
    "    *   `/data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_85078.csv` (raw downloaded GDP data CSV)\n",
    "    *   `/data/aapl_gdp_merged_cleaned.csv` (intermediate cleaned and merged data)\n",
    "    *   `/data/aapl_gdp_wrangled_features.csv` (final dataset with engineered features)\n",
    "    *   `/notebooks/data_wrangling.ipynb` (this Jupyter notebook)\n",
    "    *   `/notebooks/*.png` (saved visualization images)\n",
    "    *   `README.md`\n",
    "    *   (Optionally) `requirements.txt` listing Python package dependencies.\n",
    "4.  Uploading all these files to a GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
